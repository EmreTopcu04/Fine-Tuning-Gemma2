{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7883454,"sourceType":"datasetVersion","datasetId":4627346},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine-Tuning Gemma2 With LoRA for Turkish**\n\n## Introduction\nLanguage models such as Gemma 2 are revolutionary assets in natural language processing (NLP), facilitating applications that include text generation, language translation, and sentiment assessment. Yet, these models are frequently trained on mainly English datasets, potentially resulting in the insufficient representation of other languages and cultural subtleties. \n\nIn this notebook, my goal is to fill this gap by refining Gemma 2 for Turkish. By refining the model for Turkish, we enable communities to utilize NLP technologies customized for their language, paving the way for improved communication, education, and creativity. \n\nThis notebook is created for clarity and replicability, enabling anyone, no matter their skill level, to follow and adjust the method to suit their language or situation. \n\nBefore continuing with our notebook, I recommend you to set accelerator as GPU P100 since this notebook will work on it!\n","metadata":{}},{"cell_type":"markdown","source":"## Setup and Initialization\n\nThis section focuses on preparing the environment by installing the required libraries and importing essential modules to fine-tune Gemma 2 for Turkish. These steps ensure that all dependencies are properly installed and the environment is configured for smooth execution of the notebook.\n\n### 1. Install Dependencies\n\n- Install critical packages like `transformers`, `datasets`, `accelerate`, `peft`, and others necessary for working with large language models and performing fine-tuning tasks.\n- Use the `bitsandbytes` library for memory-efficient computations, which are essential when handling large models.\n- Configure `wandb` (Weights & Biases) for efficient experiment tracking and logging.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers datasets accelerate peft trl bitsandbytes\n%pip install googletrans==4.0.0-rc1 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:31:40.773089Z","iopub.execute_input":"2025-01-24T15:31:40.773413Z","iopub.status.idle":"2025-01-24T15:32:03.758406Z","shell.execute_reply.started":"2025-01-24T15:31:40.773386Z","shell.execute_reply":"2025-01-24T15:32:03.757402Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 2. Import Libraries\n\n- Key libraries for loading and fine-tuning the model (transformers, peft, trl) are imported.\n- Supporting libraries like torch, wandb, and datasets are also loaded to streamline model training and data handling.","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\n\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nimport os\nimport torch\nimport bitsandbytes as bnb\n\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig, setup_chat_format\n\nfrom googletrans import Translator\n\nprint(\"Importing libraries worked perfectly!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:03.759668Z","iopub.execute_input":"2025-01-24T15:32:03.759897Z","iopub.status.idle":"2025-01-24T15:32:27.083973Z","shell.execute_reply.started":"2025-01-24T15:32:03.759875Z","shell.execute_reply":"2025-01-24T15:32:27.083229Z"}},"outputs":[{"name":"stdout","text":"Importing libraries worked perfectly!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 3. Define Base Variables\n- Specify the base model (google/gemma-2-2b-it), the dataset for fine-tuning (turkish-wikipedia-dataset), and the name of the fine-tuned model (Gemma-2-2b-it-turkish).","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2\" # We are using Gemma 2\nnew_model = \"Gemma-2-2b-it-turkish\" # Fine-tuned model\nprint(\"Base variables are defined perfectly!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:27.085346Z","iopub.execute_input":"2025-01-24T15:32:27.085581Z","iopub.status.idle":"2025-01-24T15:32:27.090971Z","shell.execute_reply.started":"2025-01-24T15:32:27.085560Z","shell.execute_reply":"2025-01-24T15:32:27.090131Z"}},"outputs":[{"name":"stdout","text":"Base variables are defined perfectly!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Hardware Setup and Model Configuration\nThis section focuses on optimizing hardware and model configurations to fine-tune Gemma 2 efficiently using techniques such as Quantized LoRA (QLoRA) and Flash Attention.\n\n### 1. Check CUDA Device Capability\n- Verify the CUDA device capability to assess hardware support for advanced features like Flash Attention v2.\n- If the capability is sufficient, install and configure Flash Attention v2 to enhance memory efficiency and computation speed.\n- Set the computation data type based on GPU compatibility (e.g., bfloat16 for newer GPUs or float16 for older ones).","metadata":{}},{"cell_type":"code","source":"# Check CUDA device capability and set appropriate configurations\n\nif torch.cuda.get_device_capability()[0] >= 8:\n    # Install Flash Attention if capability allows\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16               \n    attn_implementation = \"flash_attention_2\"\n    print(\"Using bfloat16 and Flash Attention v2\")\nelse:\n    torch_dtype = torch.float16   \n    attn_implementation = \"eager\"  \n    print(\"Using float16 because of the older hardware and default attention mechanism is used\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:27.092401Z","iopub.execute_input":"2025-01-24T15:32:27.092706Z","iopub.status.idle":"2025-01-24T15:32:27.117342Z","shell.execute_reply.started":"2025-01-24T15:32:27.092684Z","shell.execute_reply":"2025-01-24T15:32:27.116640Z"}},"outputs":[{"name":"stdout","text":"Using float16 because of the older hardware and default attention mechanism is used\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 2. Configure Quantized LoRA (QLoRA)\n- Utilize BitsAndBytesConfig to enable 4-bit quantization, making it possible to load large models efficiently while maintaining performance.\n- Configure additional options, such as nf4 quantization type and double quantization, to enhance model precision and optimize computational efficiency.","metadata":{}},{"cell_type":"code","source":"# Configuration for Quantized LoRA (QLoRA)\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,                   # Enable 4-bit quantization for efficient model loading\n    bnb_4bit_quant_type=\"nf4\",           # Use NormalFloat4 (NF4) quantization\n    bnb_4bit_compute_dtype=torch_dtype,  # Using computing precision based on hardware support as we did on the upper cell\n    bnb_4bit_use_double_quant=True       # Use double quantization for improved accuracy\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:27.117980Z","iopub.execute_input":"2025-01-24T15:32:27.118168Z","iopub.status.idle":"2025-01-24T15:32:27.132634Z","shell.execute_reply.started":"2025-01-24T15:32:27.118151Z","shell.execute_reply":"2025-01-24T15:32:27.132047Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 3. Load Pretrained Model and Tokenizer\n- Load the Gemma 2 causal language model with the defined quantization and attention settings.\n- Initialize the tokenizer associated with the base model to ensure seamless integration during the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"# Load the pretrained causal language model with quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,                              # The base model identifier or path\n    quantization_config=quantization_config, # Apply QLoRA configuration\n    device_map=\"auto\",                       \n    attn_implementation=attn_implementation  # Set attention implementation\n)\n\n# Load the tokenizer corresponding to the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model,             # The base model identifier or path\n    trust_remote_code=True  # Trust custom tokenizer code if provided by the model\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:27.133420Z","iopub.execute_input":"2025-01-24T15:32:27.133699Z","iopub.status.idle":"2025-01-24T15:32:56.428427Z","shell.execute_reply.started":"2025-01-24T15:32:27.133680Z","shell.execute_reply":"2025-01-24T15:32:56.427750Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b804785e7fce414f9f37fa4d2567247a"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Configuring LoRA for Fine-Tuning\n### 1. Locate Linear Layers\n- Create a helper function to identify all 4-bit linear layers in the model that are appropriate for LoRA fine-tuning.\n- Exclude the lm_head layer to concentrate on the trainable components of the model.","metadata":{}},{"cell_type":"code","source":"def find_all_linear_names(model):\n    \"\"\"\n    This function searches for all linear layers of the 4-bit format \n    in a given model and returns their names, excluding the 'lm_head' \n    module if present.\n\n    Args:\n    - model: The model to search for linear layers in.\n\n    Returns:\n    - List of module names associated with linear layers.\n    \"\"\"\n    # The target class for linear layers (4-bit format)\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()  # Set to hold the unique names of the target linear modules\n\n    # Iterate over all named modules in the model\n    for name, module in model.named_modules():\n        # Check if the module is of the target class\n        if isinstance(module, cls):\n            names = name.split('.')  # Split the module name by dots to isolate components\n            # Add the first or last part of the name (depending on the structure) to the set\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    # Remove 'lm_head' if present in the set (needed for 16-bit models)\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n\n    # Return the list of linear module names\n    return list(lora_module_names)\n\n# Get the list of linear module names in the model\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:56.429211Z","iopub.execute_input":"2025-01-24T15:32:56.429436Z","iopub.status.idle":"2025-01-24T15:32:56.434983Z","shell.execute_reply.started":"2025-01-24T15:32:56.429413Z","shell.execute_reply":"2025-01-24T15:32:56.434138Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 2. Configure LoRA\n- Set up LoRA with key parameters such as rank (r), scaling factor (lora_alpha), and dropout rate (lora_dropout).\n- Designate the previously identified linear layers as target modules for fine-tuning.","metadata":{}},{"cell_type":"code","source":"# LoRA configuration setup\npeft_config = LoraConfig(\n    r=16,                    # Rank for LoRA\n    lora_alpha=32,           # Scaling factor for LoRA\n    lora_dropout=0.05,       # Dropout rate for LoRA\n    bias=\"none\",             # No bias in LoRA layers\n    task_type=\"CAUSAL_LM\",   # Task type for causal language modeling\n    target_modules=modules   # The list of target modules (linear layers)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:56.437053Z","iopub.execute_input":"2025-01-24T15:32:56.437299Z","iopub.status.idle":"2025-01-24T15:32:56.484448Z","shell.execute_reply.started":"2025-01-24T15:32:56.437279Z","shell.execute_reply":"2025-01-24T15:32:56.483626Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 3. Prepare Tokenizer, Chat Format, and Apply LoRA to the Model\n- Configure the tokenizer's padding side and update the chat template to prevent conflicts. Use the setup_chat_format utility to adapt the tokenizer and model for a conversational format.\n- Apply the LoRA configuration to the model, allowing efficient fine-tuning of the targeted components.","metadata":{}},{"cell_type":"code","source":"# Set the padding side for the tokenizer (important for certain models)\ntokenizer.padding_side = 'right'\n\n# Reset chat template to ensure no leftover settings\ntokenizer.chat_template = None\n\n# Setup chat format with the model and tokenizer\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Apply LoRA configurations to the model\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:56.485895Z","iopub.execute_input":"2025-01-24T15:32:56.486080Z","iopub.status.idle":"2025-01-24T15:32:59.616919Z","shell.execute_reply.started":"2025-01-24T15:32:56.486064Z","shell.execute_reply":"2025-01-24T15:32:59.615956Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Preparing and Loading the Dataset\n\nIn this section, we will focus on loading and preprocessing our dataset. Once the data is ready, we’ll format it appropriately for training and split it into training and test sets. For non-English languages like Turkish, an excellent approach to building a high-quality dataset is leveraging the 'Alpaca Dataset,' which is widely recognized for training models. To tailor it for Turkish, we will be using a translated dataset.\n\n### Introduction to Alpaca Dataset\nThe Alpaca dataset is a substantial collection of 52,000 instruction-demonstration pairs generated by OpenAI's text-davinci-003 engine, designed to improve the instruction-following capabilities of language models. This dataset is a crucial resource for instruction tuning, enabling models to better respond to user commands and queries.\n\nAlpaca comprises a diverse range of programming tasks and their corresponding instructions, representing various use cases in AI and machine learning. Key features include:\n\n- **Multiple Programming Languages:** Includes languages like Python and JavaScript, exposing models to diverse syntactic structures and paradigms.\n- **Varying Complexity:** Tasks range from simple function definitions to complex algorithm implementations, catering to different skill levels.\nData preprocessing is essential for preparing the Alpaca dataset for training. This involves addressing real-world data issues like inconsistencies and missing values, which can significantly impact model performance. Techniques such as normalization, scaling, and feature engineering improve data quality and usability. Validating the processed data ensures it meets the model's requirements, mitigating overfitting risks and enhancing overall accuracy.\n\nIn summary, the Alpaca dataset is a comprehensive resource for training AI models, especially for instruction-following tasks, contributing significantly to advancements in natural language processing.","metadata":{}},{"cell_type":"markdown","source":"### 1. Downloading the Dataset\n- Using transformers, we will be downloading 'TFLai/Turkish-Alpaca' dataset from the datasets. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"TFLai/Turkish-Alpaca\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:32:59.617943Z","iopub.execute_input":"2025-01-24T15:32:59.618318Z","iopub.status.idle":"2025-01-24T15:33:02.725101Z","shell.execute_reply.started":"2025-01-24T15:32:59.618286Z","shell.execute_reply":"2025-01-24T15:33:02.724256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/118 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b820b4a953f4b1e9146f9411ce8a09c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33cdc3b732a42edbe560b04032e3f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d82c7bcb274ee1aab429510f3d55d4"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### 2. Exploring the Original Dataset\nOnce we acquire the dataset, the next step is to examine its structure so we can determine how to format the data appropriately for training. In this section, I’ll explain the key components of the dataset: Instruction, Input, and Output.\n\n- **Instruction:** This is the core task or directive that we provide to the model. It defines the purpose or action the model should perform.\n- **Input:** This refers to any additional context or data that the model uses, provided by the instruction. The input is only included when the instruction specifies it.\n- **Output:** This is the result produced by the model, generated based on the instruction and the input (if available). It represents the model's response to the task outlined in the instruction.\n\nBy inspecting the dataset, we can see how these elements are structured, and we will adapt this structure to prepare our data for training and fine-tuning the model.","metadata":{}},{"cell_type":"code","source":"print(f\"Instruction: {ds['train'][0]['instruction']}\")\nprint(f\"Input: {ds['train'][0]['input']}\")\nprint(f\"Output: {ds['train'][0]['output']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:02.726094Z","iopub.execute_input":"2025-01-24T15:33:02.726416Z","iopub.status.idle":"2025-01-24T15:33:02.731874Z","shell.execute_reply.started":"2025-01-24T15:33:02.726393Z","shell.execute_reply":"2025-01-24T15:33:02.731058Z"}},"outputs":[{"name":"stdout","text":"Instruction: Fransa'nın başkenti nedir?\nInput: \nOutput: Fransa'nın başkenti Paris'tir.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\n\n# Extract the entire train dataset as a list of dictionaries\ndata_list = [\n    {\n        \"instruction\": example['instruction'],\n        \"input\": example['input'],\n        \"output\": example['output']\n    }\n    for example in ds['train']\n]\n\n# Write the list of dictionaries to a JSON file\nwith open('train_data.json', 'w', encoding='utf-8') as f:\n    json.dump(data_list, f, ensure_ascii=False, indent=4)\n\nprint(\"Dataset has been written to train_data.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:02.732649Z","iopub.execute_input":"2025-01-24T15:33:02.732938Z","iopub.status.idle":"2025-01-24T15:33:04.686837Z","shell.execute_reply.started":"2025-01-24T15:33:02.732908Z","shell.execute_reply":"2025-01-24T15:33:04.685933Z"}},"outputs":[{"name":"stdout","text":"Dataset has been written to train_data.json\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 3. Shrinking Down the Dataset (Optional Step)\nSince our main goal is to test how well our model will perform in a fine-tuned manner, we will shrink down our original data. ","metadata":{}},{"cell_type":"code","source":"def save_first_n_json(input_file, output_file, n=1000):\n    \"\"\"Loads a JSON file, extracts the first n entries, and saves them to a new JSON file.\n\n    Args:\n        input_file: Path to the input JSON file.\n        output_file: Path to the output JSON file.\n        n: The number of entries to extract. Defaults to 1000.\n    \"\"\"\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f_in:  # Add encoding for robustness\n            data = json.load(f_in)\n\n        if not isinstance(data, list):  # Check if the JSON data is a list\n            raise TypeError(\"The JSON data must be a list.\")\n\n\n        if len(data) < n:\n            print(f\"Warning: The input file has fewer than {n} entries. Saving all available entries.\")\n            first_n = data\n        else:\n            first_n = data[:n]\n\n        with open(output_file, 'w', encoding='utf-8') as f_out:\n            json.dump(first_n, f_out, indent=4, ensure_ascii=False)  # Use indent for pretty printing and ensure_ascii for proper UTF-8 handling\n\n        print(f\"Successfully saved the first {len(first_n)} entries to {output_file}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Input file '{input_file}' not found.\")\n    except json.JSONDecodeError:\n        print(f\"Error: Invalid JSON format in '{input_file}'.\")\n    except TypeError as e:\n        print(f\"Error: {e}\")\n\ninput_file = '/kaggle/working/train_data.json'\nnum_entries = 10000\noutput_file = f'output{num_entries}.json'\n\nsave_first_n_json(input_file, output_file, num_entries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:04.687873Z","iopub.execute_input":"2025-01-24T15:33:04.688226Z","iopub.status.idle":"2025-01-24T15:33:05.060927Z","shell.execute_reply.started":"2025-01-24T15:33:04.688166Z","shell.execute_reply":"2025-01-24T15:33:05.060202Z"}},"outputs":[{"name":"stdout","text":"Successfully saved the first 10000 entries to output10000.json\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 4. Shuffle the Dataset\n- After loading and combining the datasets, shuffle the data to introduce randomness, which helps improve generalization during training.\n- Optionally, the dataset can be truncated to a smaller subset to speed up experimentation and testing.\n\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n# Change data_files with output(num_entries).json if you shrunk it.\ndataset = load_dataset('json', data_files='/kaggle/working/train_data.json', split='train')\ndataset = dataset.shuffle()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:05.062017Z","iopub.execute_input":"2025-01-24T15:33:05.062306Z","iopub.status.idle":"2025-01-24T15:33:05.735810Z","shell.execute_reply.started":"2025-01-24T15:33:05.062284Z","shell.execute_reply":"2025-01-24T15:33:05.734981Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58868d309d16420494f69b8d4a718bbd"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"### 5. Formatting the Dataset for Chat-based Tasks\n\n- Each row of the dataset is formatted into a chat-like structure using a custom template. The format includes a \"system\" message (instruction), a \"user\" message (input question), and an \"assistant\" message (output answer). This format is then tokenized and prepared for training.","metadata":{}},{"cell_type":"code","source":"def format_chat_template(row):\n    \"\"\"\n    This function formats each row of the dataset into a chat-like structure \n    and applies the chat template for tokenization.\n\n    Args:\n    - row: The current dataset row, containing 'instruction', 'input', and 'output'.\n\n    Returns:\n    - The updated row with a 'text' field containing the formatted chat template.\n    \"\"\"\n    # Construct a JSON-like structure for the chat conversation (system, user, assistant)\n    row_json = [\n        {\"role\": \"system\", \"content\": row[\"instruction\"]},  # System message: the instruction\n        {\"role\": \"user\", \"content\": row[\"input\"]},          # User message: the input question\n        {\"role\": \"assistant\", \"content\": row[\"output\"]}     # Assistant message: the model's response\n    ]\n    # Apply the tokenizer to format the row using the chat template without tokenizing\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the chat formatting to the entire dataset using multiple processes (num_proc=4 for parallelism)\ndataset = dataset.map(format_chat_template, num_proc=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:05.736696Z","iopub.execute_input":"2025-01-24T15:33:05.736994Z","iopub.status.idle":"2025-01-24T15:33:10.711765Z","shell.execute_reply.started":"2025-01-24T15:33:05.736963Z","shell.execute_reply":"2025-01-24T15:33:10.710805Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/51914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185a79abb8a84e7189b17a504c714d0c"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### 6. Splitting the Dataset\n\n- Finally, the dataset is split into training (90%) and test (10%) sets, which will be used for fine-tuning and evaluating the model, respectively.","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training and test sets (90% train, 10% test)\ndataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:10.713133Z","iopub.execute_input":"2025-01-24T15:33:10.713446Z","iopub.status.idle":"2025-01-24T15:33:10.737329Z","shell.execute_reply.started":"2025-01-24T15:33:10.713416Z","shell.execute_reply":"2025-01-24T15:33:10.736607Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Setting Hyperparameters and Training the Model\n\nIn this section, we configure and initialize the training process using the **SFTTrainer** class, setting essential hyperparameters and training configurations for fine-tuning the model. This setup ensures an efficient and controlled training process.","metadata":{}},{"cell_type":"markdown","source":"**1. Trainer Initialization**\n\n* The **SFTTrainer** class is initialized with the model, tokenizer, training and evaluation datasets, and LoRA configuration. This class handles the entire training and evaluation workflow.\n\n**2. Hyperparameters Configuration**\n\nThe hyperparameters define how the training process will be carried out:\n\n* **Batch Size**: Both training and evaluation batches are set to 1 for memory efficiency.\n* **Gradient Accumulation**: Training steps are accumulated across two steps to simulate a larger batch size.\n* **Optimizer**: paged_adamw_32bit optimizer is used to ensure stability and efficiency.\n* **Epochs**: The model is trained for 1 epoch.\n* **Learning Rate**: A learning rate of 5e-5 is chosen to allow fine adjustments during training.\n* **Logging & Evaluation**: Training logs are saved every 10 steps, and the model is evaluated based on a set frequency.\n* **Saving Models**: The model is saved every step based on the configuration, with a maximum of two saved models.\n\n**3. Testing the Training**\n\nFor inferencing purposes, let's use a subset of the original data so that it will be easier to train!","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,                     # The model to be trained (Decreases if model size is reduced)\n    processing_class=tokenizer,      # The tokenizer used for data processing (Decreases if a simpler tokenizer is used)\n    train_dataset=dataset[\"train\"],      # Training dataset (Decreases with a smaller dataset)\n    eval_dataset=dataset[\"test\"],        # Evaluation dataset (Decreases with a smaller dataset)\n    peft_config=peft_config,         # LoRA configuration for model adaptation (Depends on LoRA setup, typically increases complexity)\n    args=SFTConfig(\n        output_dir=new_model,                                   # Directory where the trained model will be saved (No impact on computational cost)\n        per_device_train_batch_size=1,                          # Batch size for training (Decreases with smaller batch size, lower memory requirement but might reduce training efficiency)\n        per_device_eval_batch_size=1,                           # Batch size for evaluation (Same as above)\n        gradient_accumulation_steps=4,                          # Number of steps for gradient accumulation (Increases computational cost due to more steps before gradient update)\n        optim=\"paged_adamw_32bit\",                              # Optimizer type for training (Decreases with simpler optimizer, but this one should be fine)\n        num_train_epochs=1,                                     # Number of training epochs (Increases computational cost with more epochs)\n        eval_strategy=\"steps\",                                  # Evaluation strategy during training (No direct cost impact)\n        eval_steps=int(len(dataset[\"train\"]) // (1 * 2) // 5),  # Frequency of evaluation in steps (Decreases computational cost with fewer eval steps)\n        logging_steps=10,                                       # Frequency of logging during training (No significant impact unless logging too frequently)\n        warmup_steps=30,                                        # Number of steps for learning rate warmup (Decreases cost, as it prevents instability during training)\n        logging_strategy=\"steps\",                               # Logging strategy to use (No impact on computational cost)\n        learning_rate=5e-5,                                     # Learning rate for training (Decreases if learning rate is too high, as training becomes less stable)\n        save_steps=0,                                           # Frequency of saving the model in steps (Decreases computational cost by not saving frequently)\n        save_total_limit=0,                                     # Maximum number of saved models to keep (Decreases cost by reducing the need for saving models)\n        save_strategy=\"no\",                                     # Disable checkpoint saving (Decreases computational cost, as no checkpoints are saved)\n        fp16=True,                                              # Enable mixed precision (16-bit floating point) for training (Decreases computational cost due to reduced memory usage and faster computation)\n        bf16=False,                                             # Disable bfloat16 (use fp16 instead) (Decreases cost as BF16 might require more specialized hardware)\n        group_by_length=True,                                   # Group data by length for more efficient batching (Decreases computational cost by improving memory and computation efficiency)\n        report_to=\"none\",                                       # No external reporting (like to wandb) (Decreases cost as there is no extra overhead for reporting)\n        dataset_text_field=\"text\",                              # Field name for dataset text input (No impact on computational cost)\n        packing=False,                                          # Disable packing of sequences for batching (Decreases cost by avoiding extra packing computations)\n        load_best_model_at_end=False,                           # Do not load the best model after training (Decreases cost, as no additional computation is required for model selection)\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:10.738142Z","iopub.execute_input":"2025-01-24T15:33:10.738451Z","iopub.status.idle":"2025-01-24T15:33:27.110882Z","shell.execute_reply.started":"2025-01-24T15:33:10.738411Z","shell.execute_reply":"2025-01-24T15:33:27.109888Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b893673bb14400bc89b338ef2637dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5192 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba5a59501a8540adb0295b58961105b4"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"### 3. Cache Management\n\nCaching is disabled during training to avoid excessive memory usage, ensuring smooth operation on limited hardware.","metadata":{}},{"cell_type":"code","source":"# Disable caching during training to avoid memory issues\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:27.111828Z","iopub.execute_input":"2025-01-24T15:33:27.112137Z","iopub.status.idle":"2025-01-24T15:33:27.115704Z","shell.execute_reply.started":"2025-01-24T15:33:27.112104Z","shell.execute_reply":"2025-01-24T15:33:27.114940Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 4. Model Training\n\nFinally, the training process is initiated with the **train()** method, which uses the configured settings to fine-tune the model.","metadata":{}},{"cell_type":"code","source":"# Start training the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:33:27.116651Z","iopub.execute_input":"2025-01-24T15:33:27.116938Z","iopub.status.idle":"2025-01-24T21:28:51.781356Z","shell.execute_reply.started":"2025-01-24T15:33:27.116906Z","shell.execute_reply":"2025-01-24T21:28:51.780610Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11680' max='11680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11680/11680 5:55:13, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4672</td>\n      <td>1.896700</td>\n      <td>1.979302</td>\n    </tr>\n    <tr>\n      <td>9344</td>\n      <td>1.995300</td>\n      <td>1.933076</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=11680, training_loss=1.9617368704652134, metrics={'train_runtime': 21318.7879, 'train_samples_per_second': 2.192, 'train_steps_per_second': 0.548, 'total_flos': 6.626453379522509e+16, 'train_loss': 1.9617368704652134, 'epoch': 0.999957193613287})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Re-enable cache after training\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:28:51.782139Z","iopub.execute_input":"2025-01-24T21:28:51.782424Z","iopub.status.idle":"2025-01-24T21:28:51.785842Z","shell.execute_reply.started":"2025-01-24T21:28:51.782389Z","shell.execute_reply":"2025-01-24T21:28:51.785118Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Saving the Fine-Tuned Adapter Model\n\nSave the fine-tuned adapter model locally for future use and deployment.","metadata":{}},{"cell_type":"code","source":"# Save the trained model to the specified directory\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:28:51.786580Z","iopub.execute_input":"2025-01-24T21:28:51.786796Z","iopub.status.idle":"2025-01-24T21:28:58.263381Z","shell.execute_reply.started":"2025-01-24T21:28:51.786773Z","shell.execute_reply":"2025-01-24T21:28:58.262471Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Inference and Generating Responses\n\nThis section focuses on loading the fine-tuned model, configuring it for inference, and generating responses to user queries. The workflow involves preparing the model and tokenizer, formatting the input, and decoding the model's output to generate meaningful answers.","metadata":{}},{"cell_type":"markdown","source":"### 1. Clear CUDA Cache\n\nBefore inference, the CUDA memory cache is cleared to optimize GPU memory usage and prevent memory-related issues.","metadata":{}},{"cell_type":"code","source":"# Clear the CUDA memory cache.\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:28:58.264393Z","iopub.execute_input":"2025-01-24T21:28:58.264701Z","iopub.status.idle":"2025-01-24T21:28:58.714067Z","shell.execute_reply.started":"2025-01-24T21:28:58.264669Z","shell.execute_reply":"2025-01-24T21:28:58.713149Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### 2. Set Up the Model for Inference\n\nLoad the fine-tuned model with 4-bit quantization and integrate it with the base model:\n* **Quantization Configuration**: Applies 4-bit quantization to optimize memory and computational efficiency.\n* **Model Loading**: Loads the base model and fine-tuned weights, setting it to evaluation mode for inference.","metadata":{}},{"cell_type":"code","source":"# Define the path to the fine-tuned model\nnew_model_path = \"/kaggle/working/Gemma-2-2b-it-turkish\"\n\n# Configuration for 4-bit quantization to optimize model performance and memory usage\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,                     # Enable 4-bit quantization for efficient loading\n    bnb_4bit_quant_type=\"nf4\",             # Use NormalFloat4 (NF4) quantization type for better accuracy\n    bnb_4bit_compute_dtype=torch.float16,  # Use 16-bit floating-point precision for computations\n    bnb_4bit_use_double_quant=True         # Enable double quantization for improved numerical stability\n)\n\n# Load the base model with QLoRA (Quantized LoRA) configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,                              # Path or identifier of the base model\n    quantization_config=quantization_config, # Apply the quantization configuration\n    attn_implementation=\"eager\",             # Set attention mechanism implementation to \"eager\"\n    torch_dtype=torch.float16,               # Use 16-bit floating-point precision for weights and activations\n    return_dict=True,                        # Return outputs as a dictionary for better readability\n    device_map=\"auto\"                        # Automatically map model components to available devices\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:28:58.717571Z","iopub.execute_input":"2025-01-24T21:28:58.717794Z","iopub.status.idle":"2025-01-24T21:29:05.966219Z","shell.execute_reply.started":"2025-01-24T21:28:58.717776Z","shell.execute_reply":"2025-01-24T21:29:05.965414Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f23796acd394c59aa8b62dda6c9a653"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"### 3. Prepare the Tokenizer\n\nThe tokenizer is initialized to process inputs and generate outputs in a chat format. Any previous configurations are reset to avoid interference with new tasks.","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer for the base model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Reset the chat template to ensure no stale settings interfere with new tasks\ntokenizer.chat_template = None\n\n# Configure the model and tokenizer for chat-based interactions\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Load the fine-tuned model with PeftModel, applying it to the base model\nmodel = PeftModel.from_pretrained(model, new_model_path)\n\n# Set the model to evaluation mode to prepare for inference\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:29:05.967014Z","iopub.execute_input":"2025-01-24T21:29:05.967229Z","iopub.status.idle":"2025-01-24T21:29:10.742428Z","shell.execute_reply.started":"2025-01-24T21:29:05.967208Z","shell.execute_reply":"2025-01-24T21:29:10.741605Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256002, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=9216, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (rotary_emb): Gemma2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2304, out_features=256002, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"### 4. Define the Conversation and Create a Prompt\n\nFormat the conversation history into a structured prompt using the tokenizer’s chat template. This ensures the model receives well-structured input.","metadata":{}},{"cell_type":"code","source":"# Define the conversation history as a list of messages\nmessages = [\n    {\"role\": \"system\", \"content\": \"Nasılsın?\"},\n    {\"role\": \"user\", \"content\": \"\"},\n]\n\n# Apply the tokenizer's chat template to format the messages for the model\n# Set tokenize=False to avoid tokenization at this point, and add_generation_prompt=True to prepare for generation\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize the prompt and prepare the inputs for the model\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:33:40.693424Z","iopub.execute_input":"2025-01-24T21:33:40.693781Z","iopub.status.idle":"2025-01-24T21:33:40.699254Z","shell.execute_reply.started":"2025-01-24T21:33:40.693754Z","shell.execute_reply":"2025-01-24T21:33:40.698545Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### 5. Generate a Response\n\nUse the model to generate a response, applying sampling techniques for diverse and high-quality outputs:\n\n* **Top-K Sampling**: Considers the top 50 tokens at each step.\n* **Nucleus Sampling (Top-P)**: Ensures 85% cumulative probability, balancing diversity and relevance.\n* **Temperature**: A low value (0.3) makes the output more deterministic.\n* **No Repetition**: Prevents repetitive phrases by disallowing 3-gram repetitions.","metadata":{}},{"cell_type":"code","source":"# Optimized text generation with custom sampling strategies for better results\noutputs = model.generate(\n    **inputs,                # Feed the tokenized inputs to the model\n    num_return_sequences=1,  # Only return one sequence of text\n    top_k=50,                # Limit the sampling pool to the top 50 tokens\n    top_p=0.85,              # Use nucleus sampling with a cumulative probability of 85% (more deterministic output)\n    temperature=0.3,         # Lower temperature for more deterministic (less random) responses\n    no_repeat_ngram_size=3,  # Prevent repeating n-grams of size 3 (e.g., \"the the the\")\n    do_sample=True,          # Enable sampling for more diverse outputs (as opposed to greedy decoding)\n    num_beams=20             # This parameter controls the number of beams used during beam search.\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:33:43.607701Z","iopub.execute_input":"2025-01-24T21:33:43.608047Z","iopub.status.idle":"2025-01-24T21:33:46.877597Z","shell.execute_reply.started":"2025-01-24T21:33:43.608007Z","shell.execute_reply":"2025-01-24T21:33:46.876846Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### 6. Decode and Extract the Response\n\nDecode the generated output into human-readable text, cleaning unnecessary parts to extract the final response.","metadata":{}},{"cell_type":"code","source":"import re\n\n# Decode the output sequence back to text, skipping special tokens like padding and EOS markers\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract the user input\ninput_text = re.search(r\"system\\n(.*?)\\nuser\", text, re.DOTALL).group(1).strip()\n\n# Extract the assistant's response\noutput_text = re.search(r\"assistant\\n(.*)\", text, re.DOTALL).group(1).strip()\n\n# Format as Input/Output\nformatted_output = f\"Input: {input_text}\\nOutput: {output_text}\"\n\nprint(formatted_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:38:57.299092Z","iopub.execute_input":"2025-01-24T21:38:57.299458Z","iopub.status.idle":"2025-01-24T21:38:57.305649Z","shell.execute_reply.started":"2025-01-24T21:38:57.299431Z","shell.execute_reply":"2025-01-24T21:38:57.304805Z"}},"outputs":[{"name":"stdout","text":"Input: Nasılsın?\nOutput: Ben iyiyim, teşekkürler! Siz nasılsınız? Nasıl yardımcı olabilirim?\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Conclusion\n\nAs we can see, we managed to fine-tune it so that it can understand and process Turkish text. Thank you for reading this notebook. I hope I helped you learning how to fine-tune Gemma2 with LoRA for a given dataset.","metadata":{}}]}